{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Variables not used here do not need to be updated in your .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")) if os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"int-vec\")\n",
    "blob_connection_string = os.environ[\"BLOB_CONNECTION_STRING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search blob datasource connection string is optional - defaults to blob connection string\n",
    "# This field is only necessary if you are using MI to connect to the data source\n",
    "# https://learn.microsoft.com/azure/search/search-howto-indexing-azure-blob-storage#supported-credentials-and-connection-strings\n",
    "search_blob_connection_string = os.getenv(\"SEARCH_BLOB_DATASOURCE_CONNECTION_STRING\", blob_connection_string)\n",
    "blob_container_name = os.getenv(\"BLOB_CONTAINER_NAME\", \"int-vec\")\n",
    "azure_openai_endpoint = os.environ[\"ENDPOINT_URL\"]\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "azure_openai_embedding_deployment = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-ada-002\")\n",
    "azure_openai_model_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-ada-002\")\n",
    "azure_openai_model_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 1024))\n",
    "# This field is only necessary if you want to use OCR to scan PDFs in the data source\n",
    "azure_ai_services_key = os.getenv(\"AZURE_AI_SERVICES_KEY\", \"\")\n",
    "\n",
    "use_ocr = len(azure_ai_services_key) > 0\n",
    "# OCR must be used to add page numbers\n",
    "add_page_numbers = use_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup sample data in int-vec\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient  \n",
    "import glob\n",
    "\n",
    "def upload_sample_documents(\n",
    "        blob_connection_string: str,\n",
    "        blob_container_name: str,\n",
    "        documents_directory: str,\n",
    "        # Set to false if you want to use credentials included in the blob connection string\n",
    "        # Otherwise your identity will be used as credentials\n",
    "        use_user_identity: bool = True,\n",
    "    ):\n",
    "        # Connect to Blob Storage\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(logging_enable=True, conn_str=blob_connection_string, credential=DefaultAzureCredential() if use_user_identity else None)\n",
    "        container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "        if not container_client.exists():\n",
    "            container_client.create_container()\n",
    "\n",
    "        pdf_files = glob.glob(os.path.join(documents_directory, '*.pdf'))\n",
    "        for file in pdf_files:\n",
    "            with open(file, \"rb\") as data:\n",
    "                name = os.path.basename(file)\n",
    "                if not container_client.get_blob_client(name).exists():\n",
    "                    container_client.upload_blob(name=name, data=data)\n",
    "\n",
    "def upload_documents_without_ocr():\n",
    "    upload_sample_documents(\n",
    "        blob_connection_string=blob_connection_string,\n",
    "        blob_container_name=blob_container_name,\n",
    "        documents_directory=os.path.join(\"..\", \"..\", \"data\", \"documents\")\n",
    "    )\n",
    "\n",
    "def upload_documents_with_ocr():\n",
    "    upload_sample_documents(\n",
    "        blob_connection_string=blob_connection_string,\n",
    "        blob_container_name=blob_container_name,\n",
    "        documents_directory = os.path.join(\"..\", \"..\", \"data\", \"ocrdocuments\")\n",
    "    )\n",
    "\n",
    "if use_ocr:\n",
    "     upload_documents_with_ocr()\n",
    "else:\n",
    "     upload_documents_without_ocr()\n",
    "\n",
    "print(f\"Setup sample data in {blob_container_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceNotFoundError",
     "evalue": "(404) Resource not found\nCode: 404\nMessage: Resource not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m container \u001b[38;5;241m=\u001b[39m SearchIndexerDataContainer(name\u001b[38;5;241m=\u001b[39mblob_container_name)\n\u001b[1;32m     11\u001b[0m data_source_connection \u001b[38;5;241m=\u001b[39m SearchIndexerDataSourceConnection(\n\u001b[1;32m     12\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-blob\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureblob\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     data_deletion_detection_policy\u001b[38;5;241m=\u001b[39mNativeBlobSoftDeleteDeletionDetectionPolicy()\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m data_source \u001b[38;5;241m=\u001b[39m \u001b[43mindexer_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update_data_source_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_source_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData source \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_source\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created or updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/aurora/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/.virtualenvs/aurora/lib/python3.10/site-packages/azure/search/documents/indexes/_search_indexer_client.py:393\u001b[0m, in \u001b[0;36mSearchIndexerClient.create_or_update_data_source_connection\u001b[0;34m(self, data_source_connection, match_condition, skip_indexer_reset_requirement_for_cache, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m name \u001b[38;5;241m=\u001b[39m data_source_connection\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    392\u001b[0m packed_data_source \u001b[38;5;241m=\u001b[39m data_source_connection\u001b[38;5;241m.\u001b[39m_to_generated()  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_sources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_source_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacked_data_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn=representation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_indexer_reset_requirement_for_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_indexer_reset_requirement_for_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(SearchIndexerDataSourceConnection, SearchIndexerDataSourceConnection\u001b[38;5;241m.\u001b[39m_from_generated(result))\n",
      "File \u001b[0;32m~/.virtualenvs/aurora/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/.virtualenvs/aurora/lib/python3.10/site-packages/azure/search/documents/indexes/_generated/operations/_data_sources_operations.py:401\u001b[0m, in \u001b[0;36mDataSourcesOperations.create_or_update\u001b[0;34m(self, data_source_name, prefer, data_source, if_match, if_none_match, skip_indexer_reset_requirement_for_cache, request_options, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m]:\n\u001b[0;32m--> 401\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n",
      "File \u001b[0;32m~/.virtualenvs/aurora/lib/python3.10/site-packages/azure/core/exceptions.py:164\u001b[0m, in \u001b[0;36mmap_error\u001b[0;34m(status_code, response, error_map)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    163\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mResourceNotFoundError\u001b[0m: (404) Resource not found\nCode: 404\nMessage: Resource not found"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "from azure.search.documents.indexes.models import NativeBlobSoftDeleteDeletionDetectionPolicy\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(endpoint, credential)\n",
    "container = SearchIndexerDataContainer(name=blob_container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=search_blob_connection_string,\n",
    "    container=container,\n",
    "    data_deletion_detection_policy=NativeBlobSoftDeleteDeletionDetectionPolicy()\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    " \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text_data = []\n",
    " \n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        text_data.append({\"page_no\": str(page_num+1), \"text\": text})\n",
    " \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = extract_text_from_pdf('proposals/file_27.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': '1',\n",
       "  'text': 'TEAM 70:\\nDATASET 5: \\nShelf Behaviors CCTV Data  \\n'},\n",
       " {'page_no': '2',\n",
       "  'text': 'TEAM\\nMEMBERS\\nLe Thuong\\nMember\\nTrung Dung\\nTeam Leader\\nThe Vinh\\nMember\\nThanh Hao\\nMember\\nHai Yen\\nMember\\n'},\n",
       " {'page_no': '3',\n",
       "  'text': 'Agenda\\nIntroduction \\nProblem Statement\\nSolution Overview\\nMethodologies\\nCore Functionality\\nPerformance Metrics\\nTimeline and Roadmap\\nUser Interface (UI) or Interaction\\nLimitations and Future Enhancements\\nConclusion\\n1.\\n2.\\n3.\\n4.\\n5.\\n6.\\n7.\\n8.\\n9.\\n10.\\n'},\n",
       " {'page_no': '4',\n",
       "  'text': 'Proposal: Develop an AI solution utilizing video surveillance data\\n for in-depth analysis of customer behavior.\\n1.INTRO:\\nIntroduction to the importance of understanding \\ncustomer behavior in retail environments.\\n'},\n",
       " {'page_no': '5',\n",
       "  'text': 'Challenges and inefficiencies faced by retail businesses.\\nSuboptimal store layout due to a lack of understanding of customer navigation.\\nIneffective marketing strategies without detailed customer insights.\\nDecreased customer satisfaction due to a lack of understanding of needs and\\npreferences.\\n2.  PROBLEM STATEMENT\\nDEFINING THE PROBLEM\\nCore issue addressed by the MVP: Lack of real-time, \\nTraditional data collection methods are limited in scope \\n       detailed information on customer behavior in retail environments.\\n       and fail to provide a comprehensive understanding of customer interactions.\\nWEAKNESSES AND INEFFICIENCIES\\n'},\n",
       " {'page_no': '6',\n",
       "  'text': 'Current Competitive Landscape\\nCurrent Competitive Context\\nUnique Differentiation of MVP\\nOverview of the current\\ncompetitive landscape for AI-\\nsupported customer behavior\\nanalysis.\\nMention of companies utilizing\\nvideo data for monitoring\\ncustomer movements.\\nHighlight the limitations of\\nexisting solutions, primarily\\nfocusing on aggregated data\\nrather than detailed action\\nidentification\\nMVP differentiates itself by\\nfocusing on detailed action\\nrecognition and providing\\nactionable insights based on\\ncustomer behavior models.\\nAim to enable businesses to\\nmake informed decisions on store\\nlayout, product placement, and\\nmarketing strategies.\\nEmphasis on improving customer\\nsatisfaction and increasing sales.\\n'},\n",
       " {'page_no': '7',\n",
       "  'text': \"3. Solution Overview.\\nData preprocessing: Remove noise and enhance video\\nquality.\\nAction recognition model: A deep trained network to classify\\nactions.\\nAction tracking: Track customers' actions and movements\\nduring shopping.\\nOur proposed AI solution is to develop an action recognition\\nmodel to classify and track customer behavior.\\nMachine learning and computer vision techniques will be used to\\nanalyze the images.\\n\"},\n",
       " {'page_no': '8',\n",
       "  'text': \"What's new in the\\nsolution we offer?\\nExisting customer behavior analysis\\nsolutions only use sales data by month,\\nquarter, etc. This is quite inadequate\\nbecause it may not be optimal and does not\\naccurately reflect real behavior. customer's\\nhealth. We use AI tools and algorithms to\\nanalyze based on provided image and video\\ndata. The solution provides a more\\ncomprehensive and detailed behavioral\\nanalysis result.\\n\"},\n",
       " {'page_no': '9',\n",
       "  'text': 'Specifically, the AI solution will\\ninclude improvements such as:\\nDetailed action recognition: Solution to\\nidentify and classify many customer actions.\\nReal-time insights: The solution can analyze\\nand provide insights from video data in real-\\ntime, allowing businesses to make immediate\\nadjustments to their operations based on on\\ncustomer behavior.\\nActionable Insights: The solution provides\\ninsights that can be used to optimize store\\nlayout, product placement and marketing\\nstrategy.\\n'},\n",
       " {'page_no': '10',\n",
       "  'text': '4. Methodologies\\nAI MODEL ARCHITECTURE.\\nThe action recognition model is a deep convolutional\\nneural network (CNN) architecture. CNN extracts\\nfeatures from video frames and these features are then\\nused to classify customer actions.\\nMAIN COMPONENTS OF THE MODEL.\\nConvolution layer (input): This layer will receive\\nvideo data from surveillance cameras.\\nPooling (processing) layers: This layer will use\\nmachine learning techniques to analyze video data\\nand extract features of customer behavior.\\nFully connected layer (classification): This layer\\nwill use classification techniques to classify\\nbehavioral features into specific behaviors.\\nTECHNOLOGY USED\\nComputer Vision\\nMachine Learning\\nDeep Learning Frameworks: TensorFlow, PyTorch,...\\nVideo processing library: OpenCV, FFmpeg,...\\n'},\n",
       " {'page_no': '11',\n",
       "  'text': '5. The MVP will have the following core\\nfunctions:\\nAction recognition\\nAction tracking\\nAction Insights\\n'},\n",
       " {'page_no': '12',\n",
       "  'text': '1\\n2\\n3\\n4\\n5\\nAction\\nAction  recognition\\nrecognition\\nRemove the\\nproduct from\\nthe shelf\\nAccess to the\\nproduct shelf\\nPut the\\nproduct back\\non the shelf\\nProduct\\nInspection\\nPrice and\\nquality\\ncomparison\\nCustomers access\\nthe product they\\nwant to buy\\nFind and select\\nproducts they\\nwant to see\\nWhen a customer\\ndecides not to buy\\na product, they\\noften reorder the\\nproduct to its\\noriginal place.\\nCustomers review\\nthe product\\ncarefully before\\ndeciding to buy\\nCustomers often\\ncompare product\\nprice and quality\\nwith other brands\\nto ensure the best\\nvalue\\nThe MVP will be able to identify and categorize the following five\\ncustomer actions:\\n'},\n",
       " {'page_no': '13', 'text': ''},\n",
       " {'page_no': '14',\n",
       "  'text': 'Action Insights :\\nBe able to identify what factors make customers more interested and engaged.\\nAdjust your store layout to optimize the shopping experience and enhance product\\nappeal.\\nPlacing products in strategic locations in the store increases the chances that\\ncustomers will interact with the product and even increases sales, advertise and\\npromote products or areas that customers frequently visit.\\nMVPs will provide actionable insights based on customer behavior patterns :\\n'},\n",
       " {'page_no': '15',\n",
       "  'text': '6. MVPs will be evaluated based on\\nperformance metrics\\nAction recognition accuracy: The accuracy of the action recognition model will be\\nmeasured as the percentage of correctly classified frames.\\n'},\n",
       " {'page_no': '16',\n",
       "  'text': 'Accuracy of action tracking: The accuracy of the\\naction tracking component will be measured in\\nthe percentage of correctly tracked frames.\\nRelevance of action insights: will be measured\\nby how useful they are in optimizing store\\nlayouts,product placement and marketing\\nstrategy.\\nMVPs will be evaluated based on\\nperformance metrics\\n'},\n",
       " {'page_no': '17',\n",
       "  'text': 'MONTH\\nMONTH\\nMONTH\\n1-3\\n4-5\\n6\\nResearch and development of\\naction recognition models\\nData collection and action\\nrecognition model training\\nMVP implementation\\n7. Timeline and Roadmap\\nTHE MVP IS EXPECTED TO BE COMPLETED WITHIN SIX MONTHS. HERE ARE THE\\nRECOMMENDED TIMINGS AND ROADMAP:\\n'},\n",
       " {'page_no': '18',\n",
       "  'text': \"Model Development: Building on the\\nknowledge gained from research, this\\nstage focuses on developing the action\\nrecognition model. Decisions\\nregarding the model's structure,\\nalgorithms, and key parameters will be\\nmade during this phase.\\nResearch: In this phase, the team will\\ndedicate time to conduct extensive\\nresearch on action recognition methods,\\navailable machine learning models, and\\nmodern standards in this field. The goal\\nis to identify the most suitable strategy\\nand technology for the project.\\nStage 1:\\nRESEARCH AND DEVELOPMENT OF ACTION RECOGNITION MODELS(MONTH1-3)\\n\"},\n",
       " {'page_no': '19',\n",
       "  'text': 'Model Training: With the collected\\ndata, the team will initiate the model\\ntraining process. Techniques for fine-\\ntuning the model and optimizing\\nperformance will be applied during this\\nstage to ensure the model operates\\naccurately and efficiently.\\nData Collection: In this stage, the\\nteam will concentrate on gathering the\\nnecessary data for training and testing\\nthe model. This data will include\\nimages or videos of the actions that\\nthe model needs to recognize.\\nStage 2:\\nDATA COLLECTION AND TRAINING OF ACTION RECOGNITION MODEL(MONTH 4-5)\\n'},\n",
       " {'page_no': '20',\n",
       "  'text': 'Testing and Adjustment: Following\\ndeployment, the team will conduct\\ntesting to ensure that the MVP\\nfunctions correctly in real-world\\nconditions. Final adjustments may be\\nimplemented to improve performance\\nand address any issues that arise\\nduring the deployment process.\\nDeployment: The final stage of the\\nproject is the deployment of the\\nMinimum Viable Product (MVP). This\\nmay involve integrating the model into\\na real-world environment, connecting\\nwith other systems if necessary, and\\npreparing for the collection of real-\\nworld data.\\nStage 3:\\nDEPLOYMENT OF MVP(MONTH 6)\\n'},\n",
       " {'page_no': '21',\n",
       "  'text': 'Customizable Reporting:\\nEnables users to create and customize reports based on their\\nspecific needs. This may include selecting specific charts,\\ncombining data from various sources, and choosing key metrics.\\nReal-Time Interaction:\\n Allows users to track behavior and analyze data instantly,\\nenabling them to make decisions promptly. This can be achieved\\nthrough continuous updates on the user interface or through push\\nnotifications.\\nCloud Interface:\\nStores data in the cloud, providing users with access to\\ninformation from anywhere they need. This creates flexibility and\\nconvenience in managing and monitoring customer behavior.\\n8. User Interface(UI) \\nor Interaction.\\n'},\n",
       " {'page_no': '22',\n",
       "  'text': 'Light\\nAngle\\nMovement\\nThe model relies on the quality of the video.\\nTo be more specific, these factors need to be\\naccounted:\\nMODEL DRAWBACK\\n'},\n",
       " {'page_no': '23',\n",
       "  'text': 'As we know the\\ndownside of the\\nmodel, what\\ncan we do to\\nimprove it?\\nHere are five ways to optimize it.\\n'},\n",
       " {'page_no': '24',\n",
       "  'text': '9. Potential of model analysis\\nImprove with more advanced ML models to increase accuracy\\nImprove with more advanced computer vision technology\\nExpand the detectable action lists\\nImprove the ability to track more customers concurrently\\nApply personalization and customization methods from\\nbusiness to the model to analyze each customer in detail\\n'},\n",
       " {'page_no': '25',\n",
       "  'text': 'Deeply understand the\\nneeds of customers\\nBetter store organization,\\nadvertizing and promotion\\n→ more revenue/profits\\n10. Conclusion\\nHOW CAN THIS MODEL BENEFIT THE CLIENT?\\nEmotional analysis\\nBehaviour analysis\\nTime-series analysis\\n...\\n'},\n",
       " {'page_no': '26',\n",
       "  'text': '10. Conclusion\\nKEY POINTS OF MODEL DEPLOYMENT\\nHigh image/video data is crucial\\nConstantly improve the model by updating the\\nnewest ML/AI model\\nThese technologies are costly in context of time,\\nfinance and other resources, but with the right path,\\nfruitful results are on the way.\\n'},\n",
       " {'page_no': '27',\n",
       "  'text': 'Do you have\\nany questions?\\nSend it to us! We hope you\\nlearned something new.\\n'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = extract_text_from_pdf('proposals/file_27.pdf')\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEAM 70:\n",
      "DATASET 5: \n",
      "Shelf Behaviors CCTV Data  \n",
      "\n",
      "TEAM\n",
      "MEMBERS\n",
      "Le Thuong\n",
      "Member\n",
      "Trung Dung\n",
      "Team Leader\n",
      "The Vinh\n",
      "Member\n",
      "Thanh Hao\n",
      "Member\n",
      "Hai Yen\n",
      "Member\n",
      "\n",
      "Agenda\n",
      "Introduction \n",
      "Problem Statement\n",
      "Solution Overview\n",
      "Methodologies\n",
      "Core Functionality\n",
      "Performance Metrics\n",
      "Timeline and Roadmap\n",
      "User Interface (UI) or Interaction\n",
      "Limitations and Future Enhancements\n",
      "Conclusion\n",
      "1.\n",
      "2.\n",
      "3.\n",
      "4.\n",
      "5.\n",
      "6.\n",
      "7.\n",
      "8.\n",
      "9.\n",
      "10.\n",
      "\n",
      "Proposal: Develop an AI solution utilizing video surveillance data\n",
      " for in-depth analysis of customer behavior.\n",
      "1.INTRO:\n",
      "Introduction to the importance of understanding \n",
      "customer behavior in retail environments.\n",
      "\n",
      "Challenges and inefficiencies faced by retail businesses.\n",
      "Suboptimal store layout due to a lack of understanding of customer navigation.\n",
      "Ineffective marketing strategies without detailed customer insights.\n",
      "Decreased customer satisfaction due to a lack of understanding of needs and\n",
      "preferences.\n",
      "2.  PROBLEM STATEMENT\n",
      "DEFINING THE PROBLEM\n",
      "Core issue addressed by the MVP: Lack of real-time, \n",
      "Traditional data collection methods are limited in scope \n",
      "       detailed information on customer behavior in retail environments.\n",
      "       and fail to provide a comprehensive understanding of customer interactions.\n",
      "WEAKNESSES AND INEFFICIENCIES\n",
      "\n",
      "Current Competitive Landscape\n",
      "Current Competitive Context\n",
      "Unique Differentiation of MVP\n",
      "Overview of the current\n",
      "competitive landscape for AI-\n",
      "supported customer behavior\n",
      "analysis.\n",
      "Mention of companies utilizing\n",
      "video data for monitoring\n",
      "customer movements.\n",
      "Highlight the limitations of\n",
      "existing solutions, primarily\n",
      "focusing on aggregated data\n",
      "rather than detailed action\n",
      "identification\n",
      "MVP differentiates itself by\n",
      "focusing on detailed action\n",
      "recognition and providing\n",
      "actionable insights based on\n",
      "customer behavior models.\n",
      "Aim to enable businesses to\n",
      "make informed decisions on store\n",
      "layout, product placement, and\n",
      "marketing strategies.\n",
      "Emphasis on improving customer\n",
      "satisfaction and increasing sales.\n",
      "\n",
      "3. Solution Overview.\n",
      "Data preprocessing: Remove noise and enhance video\n",
      "quality.\n",
      "Action recognition model: A deep trained network to classify\n",
      "actions.\n",
      "Action tracking: Track customers' actions and movements\n",
      "during shopping.\n",
      "Our proposed AI solution is to develop an action recognition\n",
      "model to classify and track customer behavior.\n",
      "Machine learning and computer vision techniques will be used to\n",
      "analyze the images.\n",
      "\n",
      "What's new in the\n",
      "solution we offer?\n",
      "Existing customer behavior analysis\n",
      "solutions only use sales data by month,\n",
      "quarter, etc. This is quite inadequate\n",
      "because it may not be optimal and does not\n",
      "accurately reflect real behavior. customer's\n",
      "health. We use AI tools and algorithms to\n",
      "analyze based on provided image and video\n",
      "data. The solution provides a more\n",
      "comprehensive and detailed behavioral\n",
      "analysis result.\n",
      "\n",
      "Specifically, the AI solution will\n",
      "include improvements such as:\n",
      "Detailed action recognition: Solution to\n",
      "identify and classify many customer actions.\n",
      "Real-time insights: The solution can analyze\n",
      "and provide insights from video data in real-\n",
      "time, allowing businesses to make immediate\n",
      "adjustments to their operations based on on\n",
      "customer behavior.\n",
      "Actionable Insights: The solution provides\n",
      "insights that can be used to optimize store\n",
      "layout, product placement and marketing\n",
      "strategy.\n",
      "\n",
      "4. Methodologies\n",
      "AI MODEL ARCHITECTURE.\n",
      "The action recognition model is a deep convolutional\n",
      "neural network (CNN) architecture. CNN extracts\n",
      "features from video frames and these features are then\n",
      "used to classify customer actions.\n",
      "MAIN COMPONENTS OF THE MODEL.\n",
      "Convolution layer (input): This layer will receive\n",
      "video data from surveillance cameras.\n",
      "Pooling (processing) layers: This layer will use\n",
      "machine learning techniques to analyze video data\n",
      "and extract features of customer behavior.\n",
      "Fully connected layer (classification): This layer\n",
      "will use classification techniques to classify\n",
      "behavioral features into specific behaviors.\n",
      "TECHNOLOGY USED\n",
      "Computer Vision\n",
      "Machine Learning\n",
      "Deep Learning Frameworks: TensorFlow, PyTorch,...\n",
      "Video processing library: OpenCV, FFmpeg,...\n",
      "\n",
      "5. The MVP will have the following core\n",
      "functions:\n",
      "Action recognition\n",
      "Action tracking\n",
      "Action Insights\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Action\n",
      "Action  recognition\n",
      "recognition\n",
      "Remove the\n",
      "product from\n",
      "the shelf\n",
      "Access to the\n",
      "product shelf\n",
      "Put the\n",
      "product back\n",
      "on the shelf\n",
      "Product\n",
      "Inspection\n",
      "Price and\n",
      "quality\n",
      "comparison\n",
      "Customers access\n",
      "the product they\n",
      "want to buy\n",
      "Find and select\n",
      "products they\n",
      "want to see\n",
      "When a customer\n",
      "decides not to buy\n",
      "a product, they\n",
      "often reorder the\n",
      "product to its\n",
      "original place.\n",
      "Customers review\n",
      "the product\n",
      "carefully before\n",
      "deciding to buy\n",
      "Customers often\n",
      "compare product\n",
      "price and quality\n",
      "with other brands\n",
      "to ensure the best\n",
      "value\n",
      "The MVP will be able to identify and categorize the following five\n",
      "customer actions:\n",
      "\n",
      "\n",
      "Action Insights :\n",
      "Be able to identify what factors make customers more interested and engaged.\n",
      "Adjust your store layout to optimize the shopping experience and enhance product\n",
      "appeal.\n",
      "Placing products in strategic locations in the store increases the chances that\n",
      "customers will interact with the product and even increases sales, advertise and\n",
      "promote products or areas that customers frequently visit.\n",
      "MVPs will provide actionable insights based on customer behavior patterns :\n",
      "\n",
      "6. MVPs will be evaluated based on\n",
      "performance metrics\n",
      "Action recognition accuracy: The accuracy of the action recognition model will be\n",
      "measured as the percentage of correctly classified frames.\n",
      "\n",
      "Accuracy of action tracking: The accuracy of the\n",
      "action tracking component will be measured in\n",
      "the percentage of correctly tracked frames.\n",
      "Relevance of action insights: will be measured\n",
      "by how useful they are in optimizing store\n",
      "layouts,product placement and marketing\n",
      "strategy.\n",
      "MVPs will be evaluated based on\n",
      "performance metrics\n",
      "\n",
      "MONTH\n",
      "MONTH\n",
      "MONTH\n",
      "1-3\n",
      "4-5\n",
      "6\n",
      "Research and development of\n",
      "action recognition models\n",
      "Data collection and action\n",
      "recognition model training\n",
      "MVP implementation\n",
      "7. Timeline and Roadmap\n",
      "THE MVP IS EXPECTED TO BE COMPLETED WITHIN SIX MONTHS. HERE ARE THE\n",
      "RECOMMENDED TIMINGS AND ROADMAP:\n",
      "\n",
      "Model Development: Building on the\n",
      "knowledge gained from research, this\n",
      "stage focuses on developing the action\n",
      "recognition model. Decisions\n",
      "regarding the model's structure,\n",
      "algorithms, and key parameters will be\n",
      "made during this phase.\n",
      "Research: In this phase, the team will\n",
      "dedicate time to conduct extensive\n",
      "research on action recognition methods,\n",
      "available machine learning models, and\n",
      "modern standards in this field. The goal\n",
      "is to identify the most suitable strategy\n",
      "and technology for the project.\n",
      "Stage 1:\n",
      "RESEARCH AND DEVELOPMENT OF ACTION RECOGNITION MODELS(MONTH1-3)\n",
      "\n",
      "Model Training: With the collected\n",
      "data, the team will initiate the model\n",
      "training process. Techniques for fine-\n",
      "tuning the model and optimizing\n",
      "performance will be applied during this\n",
      "stage to ensure the model operates\n",
      "accurately and efficiently.\n",
      "Data Collection: In this stage, the\n",
      "team will concentrate on gathering the\n",
      "necessary data for training and testing\n",
      "the model. This data will include\n",
      "images or videos of the actions that\n",
      "the model needs to recognize.\n",
      "Stage 2:\n",
      "DATA COLLECTION AND TRAINING OF ACTION RECOGNITION MODEL(MONTH 4-5)\n",
      "\n",
      "Testing and Adjustment: Following\n",
      "deployment, the team will conduct\n",
      "testing to ensure that the MVP\n",
      "functions correctly in real-world\n",
      "conditions. Final adjustments may be\n",
      "implemented to improve performance\n",
      "and address any issues that arise\n",
      "during the deployment process.\n",
      "Deployment: The final stage of the\n",
      "project is the deployment of the\n",
      "Minimum Viable Product (MVP). This\n",
      "may involve integrating the model into\n",
      "a real-world environment, connecting\n",
      "with other systems if necessary, and\n",
      "preparing for the collection of real-\n",
      "world data.\n",
      "Stage 3:\n",
      "DEPLOYMENT OF MVP(MONTH 6)\n",
      "\n",
      "Customizable Reporting:\n",
      "Enables users to create and customize reports based on their\n",
      "specific needs. This may include selecting specific charts,\n",
      "combining data from various sources, and choosing key metrics.\n",
      "Real-Time Interaction:\n",
      " Allows users to track behavior and analyze data instantly,\n",
      "enabling them to make decisions promptly. This can be achieved\n",
      "through continuous updates on the user interface or through push\n",
      "notifications.\n",
      "Cloud Interface:\n",
      "Stores data in the cloud, providing users with access to\n",
      "information from anywhere they need. This creates flexibility and\n",
      "convenience in managing and monitoring customer behavior.\n",
      "8. User Interface(UI) \n",
      "or Interaction.\n",
      "\n",
      "Light\n",
      "Angle\n",
      "Movement\n",
      "The model relies on the quality of the video.\n",
      "To be more specific, these factors need to be\n",
      "accounted:\n",
      "MODEL DRAWBACK\n",
      "\n",
      "As we know the\n",
      "downside of the\n",
      "model, what\n",
      "can we do to\n",
      "improve it?\n",
      "Here are five ways to optimize it.\n",
      "\n",
      "9. Potential of model analysis\n",
      "Improve with more advanced ML models to increase accuracy\n",
      "Improve with more advanced computer vision technology\n",
      "Expand the detectable action lists\n",
      "Improve the ability to track more customers concurrently\n",
      "Apply personalization and customization methods from\n",
      "business to the model to analyze each customer in detail\n",
      "\n",
      "Deeply understand the\n",
      "needs of customers\n",
      "Better store organization,\n",
      "advertizing and promotion\n",
      "→ more revenue/profits\n",
      "10. Conclusion\n",
      "HOW CAN THIS MODEL BENEFIT THE CLIENT?\n",
      "Emotional analysis\n",
      "Behaviour analysis\n",
      "Time-series analysis\n",
      "...\n",
      "\n",
      "10. Conclusion\n",
      "KEY POINTS OF MODEL DEPLOYMENT\n",
      "High image/video data is crucial\n",
      "Constantly improve the model by updating the\n",
      "newest ML/AI model\n",
      "These technologies are costly in context of time,\n",
      "finance and other resources, but with the right path,\n",
      "fruitful results are on the way.\n",
      "\n",
      "Do you have\n",
      "any questions?\n",
      "Send it to us! We hope you\n",
      "learned something new.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_text = \"\\n\".join([page[\"text\"] for page in text_data])\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_to_file(text, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(text)\n",
    "        \n",
    "write_text_to_file(formatted_text, \"file_27.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader\n",
    "# from typing import Listpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'proposals/file_27.pdf', 'page': 0}, page_content='TEAM 70:\\nDATASET 5: \\nShelf Behaviors CCTV Data'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 1}, page_content='TEAM\\nMEMBERS\\nLe Thuong\\nMember\\nTrung Dung\\nTeam Leader\\nThe Vinh\\nMember\\nThanh Hao\\nMember\\nHai Yen\\nMember'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 2}, page_content='Agenda\\nIntroduction \\nProblem Statement\\nSolution Overview\\nMethodologies\\nCore Functionality\\nPerformance Metrics\\nTimeline and Roadmap\\nUser Interface (UI) or Interaction\\nLimitations and Future Enhancements\\nConclusion\\n1.\\n2.\\n3.\\n4.\\n5.\\n6.\\n7.\\n8.\\n9.\\n10.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 3}, page_content='Proposal: Develop an AI solution utilizing video surveillance data\\n for in-depth analysis of customer behavior.\\n1.INTRO:\\nIntroduction to the importance of understanding \\ncustomer behavior in retail environments.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 4}, page_content='Challenges and inefficiencies faced by retail businesses.\\nSuboptimal store layout due to a lack of understanding of customer navigation.\\nIneffective marketing strategies without detailed customer insights.\\nDecreased customer satisfaction due to a lack of understanding of needs and\\npreferences.\\n2.  PROBLEM STATEMENT\\nDEFINING THE PROBLEM\\nCore issue addressed by the MVP: Lack of real-time, \\nTraditional data collection methods are limited in scope \\n       detailed information on customer behavior in retail environments.\\n       and fail to provide a comprehensive understanding of customer interactions.\\nWEAKNESSES AND INEFFICIENCIES'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 5}, page_content='Current Competitive Landscape\\nCurrent Competitive ContextUnique Differentiation of MVP\\nOverview of the current\\ncompetitive landscape for AI-\\nsupported customer behavior\\nanalysis.\\nMention of companies utilizing\\nvideo data for monitoring\\ncustomer movements.\\nHighlight the limitations of\\nexisting solutions, primarily\\nfocusing on aggregated data\\nrather than detailed action\\nidentification\\nMVP differentiates itself by\\nfocusing on detailed action\\nrecognition and providing\\nactionable insights based on\\ncustomer behavior models.\\nAim to enable businesses to\\nmake informed decisions on store\\nlayout, product placement, and\\nmarketing strategies.\\nEmphasis on improving customer\\nsatisfaction and increasing sales.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 6}, page_content=\"3. Solution Overview.\\nData preprocessing: Remove noise and enhance video\\nquality.\\nAction recognition model: A deep trained network to classify\\nactions.\\nAction tracking: Track customers' actions and movements\\nduring shopping.\\nOur proposed AI solution is to develop an action recognition\\nmodel to classify and track customer behavior.\\nMachine learning and computer vision techniques will be used to\\nanalyze the images.\"),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 7}, page_content=\"What's new in the\\nsolution we offer?\\nExisting customer behavior analysis\\nsolutions only use sales data by month,\\nquarter, etc. This is quite inadequate\\nbecause it may not be optimal and does not\\naccurately reflect real behavior. customer's\\nhealth. We use AI tools and algorithms to\\nanalyze based on provided image and video\\ndata. The solution provides a more\\ncomprehensive and detailed behavioral\\nanalysis result.\"),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 8}, page_content='Specifically, the AI solution willinclude improvements such as:\\nDetailed action recognition: Solution to\\nidentify and classify many customer actions.\\nReal-time insights: The solution can analyze\\nand provide insights from video data in real-\\ntime, allowing businesses to make immediate\\nadjustments to their operations based on on\\ncustomer behavior.\\nActionable Insights: The solution provides\\ninsights that can be used to optimize store\\nlayout, product placement and marketing\\nstrategy.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 9}, page_content='4. Methodologies\\nAI MODEL ARCHITECTURE.\\nThe action recognition model is a deep convolutional\\nneural network (CNN) architecture. CNN extracts\\nfeatures from video frames and these features are then\\nused to classify customer actions.\\nMAIN COMPONENTS OF THE MODEL.\\nConvolution layer (input): This layer will receive\\nvideo data from surveillance cameras.\\nPooling (processing) layers: This layer will use\\nmachine learning techniques to analyze video data\\nand extract features of customer behavior.\\nFully connected layer (classification): This layer\\nwill use classification techniques to classify\\nbehavioral features into specific behaviors.\\nTECHNOLOGY USED\\nComputer Vision\\nMachine Learning\\nDeep Learning Frameworks: TensorFlow, PyTorch,...\\nVideo processing library: OpenCV, FFmpeg,...'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 10}, page_content='5. The MVP will have the following core\\nfunctions:\\nAction recognition\\nAction tracking\\nAction Insights'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 11}, page_content='1 2 3 4 5\\nActionAction  recognitionrecognition\\nRemove the\\nproduct from\\nthe shelf\\nAccess to the\\nproduct shelf\\nPut the\\nproduct back\\non the shelf\\nProduct\\nInspection\\nPrice and\\nquality\\ncomparison\\nCustomers access\\nthe product they\\nwant to buy\\nFind and select\\nproducts they\\nwant to see\\nWhen a customer\\ndecides not to buy\\na product, they\\noften reorder the\\nproduct to its\\noriginal place.\\nCustomers review\\nthe product\\ncarefully before\\ndeciding to buy\\nCustomers often\\ncompare product\\nprice and quality\\nwith other brands\\nto ensure the best\\nvalue\\nThe MVP will be able to identify and categorize the following fivecustomer actions:'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 13}, page_content='Action Insights :\\nBe able to identify what factors make customers more interested and engaged.\\nAdjust your store layout to optimize the shopping experience and enhance product\\nappeal.\\nPlacing products in strategic locations in the store increases the chances that\\ncustomers will interact with the product and even increases sales, advertise and\\npromote products or areas that customers frequently visit.\\nMVPs will provide actionable insights based on customer behavior patterns :'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 14}, page_content='6. MVPs will be evaluated based on\\nperformance metrics\\nAction recognition accuracy: The accuracy of the action recognition model will bemeasured as the percentage of correctly classified frames.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 15}, page_content='Accuracy of action tracking: The accuracy of the\\naction tracking component will be measured in\\nthe percentage of correctly tracked frames.\\nRelevance of action insights: will be measured\\nby how useful they are in optimizing store\\nlayouts,product placement and marketing\\nstrategy.\\nMVPs will be evaluated based onperformance metrics'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 16}, page_content='MONTH MONTH MONTH\\n1-3 4-5 6\\nResearch and development of\\naction recognition models\\nData collection and action\\nrecognition model training MVP implementation\\n7. Timeline and Roadmap\\nTHE MVP IS EXPECTED TO BE COMPLETED WITHIN SIX MONTHS. HERE ARE THE\\nRECOMMENDED TIMINGS AND ROADMAP:'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 17}, page_content=\"Model Development: Building on the\\nknowledge gained from research, this\\nstage focuses on developing the action\\nrecognition model. Decisions\\nregarding the model's structure,\\nalgorithms, and key parameters will be\\nmade during this phase.\\nResearch: In this phase, the team will\\ndedicate time to conduct extensive\\nresearch on action recognition methods,\\navailable machine learning models, and\\nmodern standards in this field. The goal\\nis to identify the most suitable strategy\\nand technology for the project.\\nStage 1:\\nRESEARCH AND DEVELOPMENT OF ACTION RECOGNITION MODELS(MONTH1-3)\"),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 18}, page_content='Model Training: With the collected\\ndata, the team will initiate the model\\ntraining process. Techniques for fine-\\ntuning the model and optimizing\\nperformance will be applied during this\\nstage to ensure the model operates\\naccurately and efficiently.\\nData Collection: In this stage, the\\nteam will concentrate on gathering the\\nnecessary data for training and testing\\nthe model. This data will include\\nimages or videos of the actions that\\nthe model needs to recognize.\\nStage 2:\\nDATA COLLECTION AND TRAINING OF ACTION RECOGNITION MODEL(MONTH 4-5)'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 19}, page_content='Testing and Adjustment: Following\\ndeployment, the team will conduct\\ntesting to ensure that the MVP\\nfunctions correctly in real-world\\nconditions. Final adjustments may be\\nimplemented to improve performance\\nand address any issues that arise\\nduring the deployment process.\\nDeployment: The final stage of the\\nproject is the deployment of the\\nMinimum Viable Product (MVP). This\\nmay involve integrating the model into\\na real-world environment, connecting\\nwith other systems if necessary, and\\npreparing for the collection of real-\\nworld data.\\nStage 3:\\nDEPLOYMENT OF MVP(MONTH 6)'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 20}, page_content='Customizable Reporting:\\nEnables users to create and customize reports based on their\\nspecific needs. This may include selecting specific charts,\\ncombining data from various sources, and choosing key metrics.\\nReal-Time Interaction:\\n Allows users to track behavior and analyze data instantly,\\nenabling them to make decisions promptly. This can be achieved\\nthrough continuous updates on the user interface or through push\\nnotifications.\\nCloud Interface:\\nStores data in the cloud, providing users with access to\\ninformation from anywhere they need. This creates flexibility and\\nconvenience in managing and monitoring customer behavior.\\n8. User Interface(UI) or Interaction.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 21}, page_content='Light\\nAngle\\nMovement\\nThe model relies on the quality of the video.\\nTo be more specific, these factors need to be\\naccounted:\\nMODEL DRAWBACK'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 22}, page_content='As we know thedownside of themodel, whatcan we do toimprove it?\\nHere are five ways to optimize it.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 23}, page_content='9. Potential of model analysis\\nImprove with more advanced ML models to increase accuracy\\nImprove with more advanced computer vision technology\\nExpand the detectable action lists\\nImprove the ability to track more customers concurrently\\nApply personalization and customization methods from\\nbusiness to the model to analyze each customer in detail'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 24}, page_content='Deeply understand the\\nneeds of customers\\nBetter store organization,\\nadvertizing and promotion\\n→ more revenue/profits\\n10. Conclusion\\nHOW CAN THIS MODEL BENEFIT THE CLIENT?\\nEmotional analysis\\nBehaviour analysis\\nTime-series analysis\\n...'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 25}, page_content='10. Conclusion\\nKEY POINTS OF MODEL DEPLOYMENT\\nHigh image/video data is crucial\\nConstantly improve the model by updating the\\nnewest ML/AI model\\nThese technologies are costly in context of time,\\nfinance and other resources, but with the right path,\\nfruitful results are on the way.'),\n",
       " Document(metadata={'source': 'proposals/file_27.pdf', 'page': 26}, page_content='Do you haveany questions?\\nSend it to us! We hope you\\nlearned something new.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"proposals/file_27.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TEAM 70:\\nDATASET 5: \\nShelf Behaviors CCTV Data TEAM\\nMEMBERS\\nLe Thuong\\nMember\\nTrung Dung\\nTeam Leader\\nThe Vinh\\nMember\\nThanh Hao\\nMember\\nHai Yen\\nMember Agenda\\nIntroduction \\nProblem Statement\\nSolution Overview\\nMethodologies\\nCore Functionality\\nPerformance Metrics\\nTimeline and Roadmap\\nUser Interface (UI) or Interaction\\nLimitations and Future Enhancements\\nConclusion\\n1.\\n2.\\n3.\\n4.\\n5.\\n6.\\n7.\\n8.\\n9.\\n10. Proposal: Develop an AI solution utilizing video surveillance data\\n for in-depth analysis of customer behavior.\\n1.INTRO:\\nIntroduction to the importance of understanding \\ncustomer behavior in retail environments. Challenges and inefficiencies faced by retail businesses.\\nSuboptimal store layout due to a lack of understanding of customer navigation.\\nIneffective marketing strategies without detailed customer insights.\\nDecreased customer satisfaction due to a lack of understanding of needs and\\npreferences.\\n2.  PROBLEM STATEMENT\\nDEFINING THE PROBLEM\\nCore issue addressed by the MVP: Lack of real-time, \\nTraditional data collection methods are limited in scope \\n       detailed information on customer behavior in retail environments.\\n       and fail to provide a comprehensive understanding of customer interactions.\\nWEAKNESSES AND INEFFICIENCIES Current Competitive Landscape\\nCurrent Competitive ContextUnique Differentiation of MVP\\nOverview of the current\\ncompetitive landscape for AI-\\nsupported customer behavior\\nanalysis.\\nMention of companies utilizing\\nvideo data for monitoring\\ncustomer movements.\\nHighlight the limitations of\\nexisting solutions, primarily\\nfocusing on aggregated data\\nrather than detailed action\\nidentification\\nMVP differentiates itself by\\nfocusing on detailed action\\nrecognition and providing\\nactionable insights based on\\ncustomer behavior models.\\nAim to enable businesses to\\nmake informed decisions on store\\nlayout, product placement, and\\nmarketing strategies.\\nEmphasis on improving customer\\nsatisfaction and increasing sales. 3. Solution Overview.\\nData preprocessing: Remove noise and enhance video\\nquality.\\nAction recognition model: A deep trained network to classify\\nactions.\\nAction tracking: Track customers' actions and movements\\nduring shopping.\\nOur proposed AI solution is to develop an action recognition\\nmodel to classify and track customer behavior.\\nMachine learning and computer vision techniques will be used to\\nanalyze the images. What's new in the\\nsolution we offer?\\nExisting customer behavior analysis\\nsolutions only use sales data by month,\\nquarter, etc. This is quite inadequate\\nbecause it may not be optimal and does not\\naccurately reflect real behavior. customer's\\nhealth. We use AI tools and algorithms to\\nanalyze based on provided image and video\\ndata. The solution provides a more\\ncomprehensive and detailed behavioral\\nanalysis result. Specifically, the AI solution willinclude improvements such as:\\nDetailed action recognition: Solution to\\nidentify and classify many customer actions.\\nReal-time insights: The solution can analyze\\nand provide insights from video data in real-\\ntime, allowing businesses to make immediate\\nadjustments to their operations based on on\\ncustomer behavior.\\nActionable Insights: The solution provides\\ninsights that can be used to optimize store\\nlayout, product placement and marketing\\nstrategy. 4. Methodologies\\nAI MODEL ARCHITECTURE.\\nThe action recognition model is a deep convolutional\\nneural network (CNN) architecture. CNN extracts\\nfeatures from video frames and these features are then\\nused to classify customer actions.\\nMAIN COMPONENTS OF THE MODEL.\\nConvolution layer (input): This layer will receive\\nvideo data from surveillance cameras.\\nPooling (processing) layers: This layer will use\\nmachine learning techniques to analyze video data\\nand extract features of customer behavior.\\nFully connected layer (classification): This layer\\nwill use classification techniques to classify\\nbehavioral features into specific behaviors.\\nTECHNOLOGY USED\\nComputer Vision\\nMachine Learning\\nDeep Learning Frameworks: TensorFlow, PyTorch,...\\nVideo processing library: OpenCV, FFmpeg,... 5. The MVP will have the following core\\nfunctions:\\nAction recognition\\nAction tracking\\nAction Insights 1 2 3 4 5\\nActionAction  recognitionrecognition\\nRemove the\\nproduct from\\nthe shelf\\nAccess to the\\nproduct shelf\\nPut the\\nproduct back\\non the shelf\\nProduct\\nInspection\\nPrice and\\nquality\\ncomparison\\nCustomers access\\nthe product they\\nwant to buy\\nFind and select\\nproducts they\\nwant to see\\nWhen a customer\\ndecides not to buy\\na product, they\\noften reorder the\\nproduct to its\\noriginal place.\\nCustomers review\\nthe product\\ncarefully before\\ndeciding to buy\\nCustomers often\\ncompare product\\nprice and quality\\nwith other brands\\nto ensure the best\\nvalue\\nThe MVP will be able to identify and categorize the following fivecustomer actions: Action Insights :\\nBe able to identify what factors make customers more interested and engaged.\\nAdjust your store layout to optimize the shopping experience and enhance product\\nappeal.\\nPlacing products in strategic locations in the store increases the chances that\\ncustomers will interact with the product and even increases sales, advertise and\\npromote products or areas that customers frequently visit.\\nMVPs will provide actionable insights based on customer behavior patterns : 6. MVPs will be evaluated based on\\nperformance metrics\\nAction recognition accuracy: The accuracy of the action recognition model will bemeasured as the percentage of correctly classified frames. Accuracy of action tracking: The accuracy of the\\naction tracking component will be measured in\\nthe percentage of correctly tracked frames.\\nRelevance of action insights: will be measured\\nby how useful they are in optimizing store\\nlayouts,product placement and marketing\\nstrategy.\\nMVPs will be evaluated based onperformance metrics MONTH MONTH MONTH\\n1-3 4-5 6\\nResearch and development of\\naction recognition models\\nData collection and action\\nrecognition model training MVP implementation\\n7. Timeline and Roadmap\\nTHE MVP IS EXPECTED TO BE COMPLETED WITHIN SIX MONTHS. HERE ARE THE\\nRECOMMENDED TIMINGS AND ROADMAP: Model Development: Building on the\\nknowledge gained from research, this\\nstage focuses on developing the action\\nrecognition model. Decisions\\nregarding the model's structure,\\nalgorithms, and key parameters will be\\nmade during this phase.\\nResearch: In this phase, the team will\\ndedicate time to conduct extensive\\nresearch on action recognition methods,\\navailable machine learning models, and\\nmodern standards in this field. The goal\\nis to identify the most suitable strategy\\nand technology for the project.\\nStage 1:\\nRESEARCH AND DEVELOPMENT OF ACTION RECOGNITION MODELS(MONTH1-3) Model Training: With the collected\\ndata, the team will initiate the model\\ntraining process. Techniques for fine-\\ntuning the model and optimizing\\nperformance will be applied during this\\nstage to ensure the model operates\\naccurately and efficiently.\\nData Collection: In this stage, the\\nteam will concentrate on gathering the\\nnecessary data for training and testing\\nthe model. This data will include\\nimages or videos of the actions that\\nthe model needs to recognize.\\nStage 2:\\nDATA COLLECTION AND TRAINING OF ACTION RECOGNITION MODEL(MONTH 4-5) Testing and Adjustment: Following\\ndeployment, the team will conduct\\ntesting to ensure that the MVP\\nfunctions correctly in real-world\\nconditions. Final adjustments may be\\nimplemented to improve performance\\nand address any issues that arise\\nduring the deployment process.\\nDeployment: The final stage of the\\nproject is the deployment of the\\nMinimum Viable Product (MVP). This\\nmay involve integrating the model into\\na real-world environment, connecting\\nwith other systems if necessary, and\\npreparing for the collection of real-\\nworld data.\\nStage 3:\\nDEPLOYMENT OF MVP(MONTH 6) Customizable Reporting:\\nEnables users to create and customize reports based on their\\nspecific needs. This may include selecting specific charts,\\ncombining data from various sources, and choosing key metrics.\\nReal-Time Interaction:\\n Allows users to track behavior and analyze data instantly,\\nenabling them to make decisions promptly. This can be achieved\\nthrough continuous updates on the user interface or through push\\nnotifications.\\nCloud Interface:\\nStores data in the cloud, providing users with access to\\ninformation from anywhere they need. This creates flexibility and\\nconvenience in managing and monitoring customer behavior.\\n8. User Interface(UI) or Interaction. Light\\nAngle\\nMovement\\nThe model relies on the quality of the video.\\nTo be more specific, these factors need to be\\naccounted:\\nMODEL DRAWBACK As we know thedownside of themodel, whatcan we do toimprove it?\\nHere are five ways to optimize it. 9. Potential of model analysis\\nImprove with more advanced ML models to increase accuracy\\nImprove with more advanced computer vision technology\\nExpand the detectable action lists\\nImprove the ability to track more customers concurrently\\nApply personalization and customization methods from\\nbusiness to the model to analyze each customer in detail Deeply understand the\\nneeds of customers\\nBetter store organization,\\nadvertizing and promotion\\n→ more revenue/profits\\n10. Conclusion\\nHOW CAN THIS MODEL BENEFIT THE CLIENT?\\nEmotional analysis\\nBehaviour analysis\\nTime-series analysis\\n... 10. Conclusion\\nKEY POINTS OF MODEL DEPLOYMENT\\nHigh image/video data is crucial\\nConstantly improve the model by updating the\\nnewest ML/AI model\\nThese technologies are costly in context of time,\\nfinance and other resources, but with the right path,\\nfruitful results are on the way. Do you haveany questions?\\nSend it to us! We hope you\\nlearned something new.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(list(map(lambda page: page.page_content, pages)))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "def upload_file_to_azure(file_path, container_name=os.getenv(\"CONTAINER_NAME\"), connection_string=os.getenv(\"BLOB_CONNECTION_STRING\")):\n",
    "    # Create a BlobServiceClient using the connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(\n",
    "        connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # Get the blob client\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "    # Upload the file\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data)\n",
    "\n",
    "    print(\n",
    "        f\"File {file_path} uploaded to Azure Blob Storage in container {container_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File file_27..txt uploaded to Azure Blob Storage in container extracted-data.\n"
     ]
    }
   ],
   "source": [
    "upload_file_to_azure('file_27.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aurora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
